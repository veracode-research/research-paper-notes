# Interesting things from Code:


## [SWAN](http://lisanqd.com/wp-content/uploads/2019/07/issta19.pdf) & [SWANAssist](http://lisanqd.com/wp-content/uploads/2019/08/ase19tool.pdf) - [Code base](https://github.com/secure-software-engineering/swan)

SWAN is a machine learning based tool, which outputs SRM (Security Relevant Methods a.k.a sources, sinks, validators & authentication methods) in both their own code base and libraries they use. This output should be used in configuring other static analysis tool, to make them intelligent. Also, sinks are CWE categorized. 

Features and Training data, are constructed more formally going by how a typical java projects are written. [dataset](https://github.com/secure-software-engineering/swan/blob/master/swan_core/src/main/resources/input/TrainDataMethods/configurationmethods.json) 

SVM classifier is being used from [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/index.html) library. 

[Soot](http://sable.github.io/soot/) framework is being used for call graph analysis.

SWANAssist is an IntelliJ plugin, which is mainly a UI for how to use SWAN's output for actual static analysis. Further, it also gives developer ability to make edits (add or remove methods) to the training set. Developer can manually tag any missing sources/sinks/validators on code base we working on. This renewed training set is fed back to analysis, thru the plugin for more accurate analysis. Plugin is demo [here](https://www.youtube.com/watch?v=fSyD3V6EQOY&feature=youtu.be)

**Thoughts:** 

- How many commercial/open source static analysis tool are so easily configurable ?
- No entry points modeling or propogators are present.
- Features are more intelligently determined based on how typical java projects are written, focused towards tagging, CWE categorization, FP reduction etc. 
- With a dataset size of just 256, they are making such claims... If its working, its very motivating :)

**Summary:** If we decide to automate source/sink/validator tagging of 3rd party libraries (which don't need much of entry points modeling), feature modeling used in this paper could be an interesting approach. Would need to extend type declarations further to be more inclusive of different CWE categorizations and languages. We could further extend this approach to detect custom cleansers automatically, if developer codebase is following typical java projects conventions.

## [Machine Learning on Source Code](https://www.dotconferences.com/2018/05/vadim-markovtsev-machine-learning-on-source-code) - Talk at dotconference.

Talk by Vadim Markovtsev, goes by hashtag MLOnCode... One of the biggest contributor of ML on source code. 

This talk is about, smart auto completion in IDEs. If a source code has a method named get, and if a developer is adding another method starting with "s", it would show "set".

It goes by the concept of "code naturalness", which is hypothetically code writters are humans speaking human languages, which gets translated into machine languages. They are trying to do "identifier embeddings" by guessing class name/method names. 

Their training corpus, is humongous ~200k github repos. To find distance matrix on such a huge vocabulary and further matrix multiplications is completely infeasible. Thus, the came up with an algorithm [Swivel](https://github.com/src-d/tensorflow-swivel) implemented on Tensorflow. This mainly parses AST, at individual node/subnode level, making distance computations using co-occurance matrix more contextual. This algorithm is explained in detail [here](https://arxiv.org/pdf/1602.02215.pdf).

The explanation of how they setup their pipeline of fetching/cloning github repos could be useful as well, if we ever get into the business of dealing with 1000s of repos.


**Thoughts:**

- word2vec could be sufficient for datasets within 50GB, which doesnt lead onto huge vocabularies. If our dataset grows above that, worth exploring this representation learning technique.
- Dealing with 1000s of repos is a real problem, these guys have open sourced their pipeline [PGA](https://github.com/src-d/datasets/tree/master/PublicGitArchive), to help us with it.

## [Modeling Vocabulary for Big Code Machine Learning](https://arxiv.org/pdf/1904.01873.pdf)

The paper is going towards exploring if transfer learning (pretraining models), can be done on source code. In their initial stages they found that biggest problem towards this goal would be the huge size of vocabulary which would be encountered since identifiers could be anything. They have discussed various choices around controlling the size of vocabulary and also modeling choices in the paper. Further, they have presented results on training time and some basic tasks of what the trained model could do. They are pretraining models using LSTM on corpus generated by [Miltiadis Allamanis](http://groups.inf.ed.ac.uk/cup/javaGithub/)

Thoughts/Summary: Experiments and summaries provided for each choices of both vocabulary/models were great. May be we can learn from the choices and experiment for our own source code based models.

## [Automated Vulnerability Detection in Source Code Using Deep Representation Learning](https://arxiv.org/pdf/1807.04320.pdf):
 
Their data set is picking C/C++ code from open sources (github, debian etc), and labeling each function based on 5 CWE categories using static analysis tools. They haven't discussed labeling aspect very clearly, which might have been interesting. [Dataset](https://osf.io/q7dyc/) is open sourced though. Using this dataset, they are predicting vulnerabilities using CNN. 
	
**Thoughts:**
	- Saw little advantage over Veracode's static analysis platform.

## [Discovering vulnerabilities using data-flow analysis and machine learning](https://dspace.ou.nl/bitstream/1820/9725/1/Kronjee%20J%20IM9906%20AF%20scriptie.pdf)

Downloaded white paper from ShiftLeft, which talks about "Semantic Code Property Graph" which they build from various program analysis structures (AST, CFGs etc) and than using Gremlin (graph traversal language), find vulnerabilities. They have various cool open sourced stuff on their website. In [this](https://www.oreilly.com/ideas/how-machine-learning-can-be-used-to-write-more-secure-computer-programs) podcast Fabian Yamaguchi their chief scientist does a good job in discussing this whole approach.

**Thoughts:**
	- Saw little advantage over Veracode's static analysis platform.

	
## [Deep API Learning](https://arxiv.org/pdf/1605.08535.pdf): 

This guys, convert natural language queries to real world code sniplets to be used directly in code. For e.g. 
"Parse XML Files in java" -> " DocumentBuilderFactory.newInstance 
 											DocumentBuilderFactory.newDocumentBuilder 
 											DocumentBuilder.parse"
 											
 
They have also open sourced some of their toolchain [here](https://guxd.github.io/deepapi/)			 
	


## [Mining Fix Patterns for FindBugs Violations](https://arxiv.org/pdf/1712.03201.pdf)
 
  - True Positives are flaws which get fixed.
  - Flaw mitigation == if a flaw is found in revision1 and absent in revision2, after a code change. Findbugs is run on revision1 and revision2.
  - [Dataset generation tool](https://github.com/FixPattern/findbugs-violations): Going thru their dataset, they mainly parsed just quality issues, not many (may be none), security issues.
  - By running FindBugs on each and every revision of build, for each violation instance collecting violation type, the enclosing program entity (e.g., project, class or method), the commit id, the file path, and the location (i.e., start and end line numbers) where the violation is detected. Lot of details around fixed/unfixed code patterns and structure of a code pattern. 

 **Thoughts:**
 - We might have a better approach to mine thru security flaw fixes. 
 - Once we figure out above step, structuring the dataset could be inspired from this paper.

## [Microsoft's IntelliCode](https://visualstudio.microsoft.com/services/intellicode/) & [DeepCode](https://marketplace.visualstudio.com/items?itemName=DeepCode.deepcode)

# Comment Generation:

## [Deep Code Comment Generation](https://github.com/sriniiyer/codenn/blob/master/summarizing_source_code.pdf)

- Uses RNN based on LSTMs attention to model source code to Natural Language

## [Summarizing Source Code using a neural attention model](https://github.com/sriniiyer/codenn/blob/master/summarizing_source_code.pdf):
- Code-NN tool, summarized C# and sql code bases.
- Dataset == code sniplets from stackover (accepted answers), title of post added to training data corpus.
- Works on unstructured form of code. Not a huge fan!

## [Improving Automatic Source Code Summarization via Deep Reinforcement Learning](https://arxiv.org/pdf/1811.07234.pdf)

## [Code Summarization](https://github.com/src-d/awesome-machine-learning-on-source-code#code-summarization)


# Vulnerability Detection:
	
## [Automated software vulnerability detection with machine learning](https://arxiv.org/pdf/1803.04497.pdf):
 

 
## [Summary of vulnerability related technologies based on machine learning](https://aip.scitation.org/doi/pdf/10.1063/1.5033718):

## [An Automated Vulnerability Detection and Remediation Method for Software Security](http://www.mdpi.com/2071-1050/10/5/1652/pdf)

## [Using machine learning to detect software vulnerabilities](https://techxplore.com/news/2018-07-machine-software-vulnerabilities.html):
- [Microsoft uses machine learning to combat security vulnerabilities](https://sdtimes.com/deep-neural-networks/microsoft-uses-machine-learning-combat-security-vulnerabilities/)
	
# Text Classification:

## [Universal Language Model Fine Tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf)		

	

# Adversarial Machine Learning:
							

## [TreeHuggr: Discovering where tree-based classifiers are vulnerable to adversarial attack](https://www.camlis.org/bobby-filar): 
Attacks on actual ML datasets.





# References
-  [Cool links & research papers related to Machine Learning applied to source code (MLonCode)](https://github.com/src-d/awesome-machine-learning-on-source-code#code-summarization)
